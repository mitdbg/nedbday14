<html>

<head>

<script type="text/javascript">

function toggle(element) {

 if (document.getElementById(element).style.display == "none") {

 document.getElementById(element).style.display = "";

 } else {

 document.getElementById(element).style.display = "none";

 }

}

</script>

<title>New England Database Summit Posters</title>
<style type="text/css">
h2 {
padding-left: 0em;
 color : #0000CC;

}

title {
}

authors {
}

abstract {
}

BODY {
padding-left: 16em;
 background-color : #FFFFFF;
 font-family : Verdana, Geneva, Arial, Helvetica, sans-serif;
 color : #000088;
 margin: 0px; 
 width: 700px;
 }
 ul.navbar {
 position: absolute;
 top: 0em;
 left: 1em;
 width: 11em }

</style>

</head>
<body bgcolor=#FFFFFF fgcolor=#000000>

<table>
<tr>
<td></td>
</tr>
<tr>
<td>
<h2>
New England Database Summit Posters <br> 
</h2>

The poster session will include drinks and appetizers. It will be held in on the 9th floor of building 32.  Take the elevators in the Gates Tower to the 9th floor.

</h2>

<h3> Information for Poster Presenters </h3>

We will be able to provide poster board, easels, and mounting supplies for
you.
<p>
You will need to print your poster and bring it with you to the conference. 
We recommend posters to be either A0, A1, or ANSI D or E sizes (either 24" by
32" or 36" by 42"). Our poster boards are large enough to accommodate any of these.
You may orient your poster as you see fit. If you prefer to print out individual 8.5" x 11" pages,
our board will be large enough to accommodate about 12 pages. However, we recommend making
a single poster -- if you don't have access to a large format printer Fedex/Kinkos can print your
poster for you.
<p>
We will have a storage area for poster tubes at the conference. You will be able to set up
your posters during the lunch break.



<h3>
List of accepted posters:
</h3>


<!-- Site navigation menu -->
<ul class="navbar">
              <img src="newengland.jpg" width="200">
              <li><a href="http://db.csail.mit.edu/nedbday14">Home page</a></li>
              <li><a href="program.html">Program</a> </li>
              <li><a href="papers.html">Accepted Papers</a> </li>
              <li><a href="posters.html">Accepted Posters</a> </li>
	                 <li><a href="http://www.eventbrite.com/e/new-england-database-day-summit-tickets-9897331184">Registration</a>
              <li><a href="https://cmt.research.microsoft.com/NEDB2014/">Submission Site</a>
              </li>
              <li><a href="http://whereis.mit.edu/map-jpg?selection=32">Map</a>
              </li>
            </ul>

<ul>
 <li>Alekh Jindal*, MIT; Jorge Quiane, ; Sam Madden, MIT<br/><strong>Flexible Data Storage for Big Data </strong><a href="javascript:toggle(39)"><br/>Click to show abstract. </a> <div id="39" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">The Hadoop framework has been widely adopted by the industry as well as the research community for big data analytics. As a result, Hadoop has also been the subject of many research works to cope with a number of performance issues. These optimizations, however, come at a price. They make deep changes in the Hadoop code. As a result, they are specialized to particular workloads, incompatibles with newer versions of Hadoop, and hard to use. In this poster, we present Cartilage, a flexible data storage framework designed to ease deployment and development of extensions to distributed storage and processing frameworks like Hadoop. Cartilage sits on top of an existing HDFS and takes care of mapping user datasets to HDFS files. Cartilage users can program their MapReduce jobs using the standard Hadoop operators. In addition to MapReduce, Cartilage is also integrated with Apache Spark and allows users to process their Cartilage datasets from the Spark engine.</div><p></li>
 <li>Alvin Cheung*, MIT CSAIL; Sam Madden, MIT; Armando Solar-Lezama, MIT CSAIL<br/><strong>Sloth: Using Lazy Evaluation for Query Batching </strong><a href="javascript:toggle(8)"><br/>Click to show abstract. </a> <div id="8" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">(please refer to pdf contents)</div><p></li>
 <li>Andrew Crotty*, Brown University; Alex Galakatos, Brown; Kayhan Dursun, Brown University<br/><strong>TupleWare: A Main Memory Platform for Next Generation Analytics </strong><a href="javascript:toggle(41)"><br/>Click to show abstract. </a> <div id="41" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">The rapidly evolving field of data science is at the precipice of a fundamental shift in the meaning of analytics. Highly complex computations have come to define the typical workload, with jobs ranging from machine learning to large-scale visualization. With the need to express these increasingly complex workflows, the UDF has come to serve as the core of the processing pipeline. However, since existing systems fall far short when dealing with these classes of problems, we propose TupleWare, a distributed, main memory platform for big data analytics. TupleWare is the first system to integrate the LLVM compiler framework in order to achieve (1) language-independence for complex analytics workloads, (2) the ability to seamlessly merge UDFs with control flow statements, and (3) unique opportunities to examine the internals of UDFs and apply hardware-dependent optimizations on a case-by-case basis. This poster provides an overview of the TupleWare architecture and preliminary benchmarks against existing systems.</div><p></li>
 <li>Bahar Qarabaqi*, Northeastern University; Mirek Riedewald, Northeastern University<br/><strong>User-Driven Refinement of Imprecise Queries </strong><a href="javascript:toggle(18)"><br/>Click to show abstract. </a> <div id="18" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">We propose techniques for exploratory search in large databases. The goal is to provide new functionality that aids users in homing in on the right query conditions to find what they are looking for. Query refinement proceeds interactively by repeatedly consulting the user to manage query conditions. This process is characterized by three key challenges: (1) dealing with incomplete and imprecise user input, (2) keeping user effort low, and (3) guaranteeing interactive system response time. We address the first two challenges with a probability-based framework that guides the user to the most important query conditions. To recover from input errors, we introduce the notion of sensitivity and propose efficient algorithms for identifying the most sensitive user input, i.e., those inputs that had the greatest influence on the query results. For the third challenge, we develop techniques that can deliver estimates of the required probabilities within a given hard realtime limit and are able to adapt automatically as the interactive query refinement proceeds.</div><p></li>
 <li>Boduo Li*, UMass Amherst; Yunmeng Ban, UMass Amherst; Edward Mazur, UMass Amherst; Yanlei Diao, ; Andrew McGregor, UMass Amherst; Prashant Shenoy, UMass Amherst<br/><strong>A Platform for Scalable Low-Latency Analytics using MapReduce </strong><a href="javascript:toggle(43)"><br/>Click to show abstract. </a> <div id="43" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">MapReduce has gained tremendous popularity as a scalable, easy-to-use parallel programming model for large-scale data analytics. However, in the MapReduce model, data needs to be loaded to the cluster before any queries can be run, resulting in a high delay to start query processing. Moreover, answers to a long-running query are returned only when the entire job completes, causing a long delay in returning query answers. Such delays are undesirable for interactive analysis where users are waiting online for answers, or monitoring and decision-making problems that are often under time constraints. In this proposal, we design, develop, and evaluate a scalable analytics platform that fundamentally transforms the existing cluster computing paradigm into an incremental parallel processing paradigm, and further extends to low-latency analytics. Our empirical and theoretical analyses of Hadoop-based MapReduce systems show that the widely-used sort-merge implementation for partitioning and parallel processing poses a fundamental barrier to incremental analytics, despite various optimizations. To address such limitation, we employ hash techniques to enable fast in-memory incremental processing, and frequent key based techniques to extend such processing to workloads that require a key-state space exceeding available memory. Our further benchmarks show that high latency may occur in several phases due to unbalanced resource allocation and the latency-agnostic prioritization in processing. We propose to address these problems by optimizing system configuration using a new cost model and employing latency-aware scheduling techniques in a revised MapReduce architecture.</div><p></li>
 <li>Chuan Lei*, WPI; Elke Rundensteiner, WPI; Mohamed Eltabakh, WPI<br/><strong>Redoop: Supporting Recurring Queries in Hadoop </strong><a href="javascript:toggle(13)"><br/>Click to show abstract. </a> <div id="13" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">The growing demand for large-scale data analytics ranging from online advertisement placement, log processing, to fraud detection, has led to the design of highly scalable data-intensive computing infrastructures such as the Hadoop platform. Recurring queries, repeatedly being executed for long periods of time on rapidly evolving high-volume data, have become a bedrock component in most of these analytic applications. Despite their importance, the plain Hadoop along with its state-of-art extensions lack built-in support for recurring queries. In particular, they lack efficient and scalable analytics over evolving datasets. In this work, we present the Redoop system, an extension of the Hadoop framework, designed to fill in this void. Redoop supports recurring queries as firstclass citizen in Hadoop without sacrificing any of its core features. More importantly, Redoop deploys innovative window-aware optimization techniques for recurring query execution including adaptive window-aware data partitioning, window-aware task scheduling, and inter-window caching mechanisms. Redoop retains the fault-tolerance of MapReduce via automatic cache recovery and task re-execution support. Our extensive experimental study with real datasets demonstrates that Redoop achieves significant runtime performance gains of up to 9x speedup compared to the plain Hadoop.</div><p></li>
 <li>Dongqing Xiao*, WPI<br/><strong>InsightNotes:A Scalable Summary-based Annotation Management </strong><a href="javascript:toggle(9)"><br/>Click to show abstract. </a> <div id="9" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">Scientific database systems provide backbone support to various scientific appli cations. In these applications, efficient and effective annotation management mechanism is vital for sharing knowledge and establishing a collaborative environment amon g end-users and scientists. Annotations may represent comments on the data, provenance or lineage information, and highlights on conflicting or erroneous values. The extensive use of annotation and the expanding scale of collaboration may cau se the size of annotations to far exceed the size of the original data, and hence it becomes extremely difficult for end-users to extract useful insight and the valuable knowledge hidden within the annotations. In this project, we propose the InsightNotes system; an advanced annotation mana gement system over relational databases, for exploiting annotations in novel ways throu gh summarization, mining, and ranking techniques with the objective of reporting concise and meaningful representations instead of the raw annotations. InsightNotes also addresses the query processing challenges involved in building and querying such complex representations. </div><p></li>
 <li>Haopeng Zhang*, UMass Amherst<br/><strong>On Complexity and Optimization of Expensive Queries in Complex Event Processing </strong><a href="javascript:toggle(33)"><br/>Click to show abstract. </a> <div id="33" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">Pattern queries are widely used in complex event processing (CEP) systems. Existing pattern matching techniques, however, can provide only limited performance for expensive queries in real-world applications, which may involve Kleene closure patterns, flexible event selection strategies, and events with imprecise timestamps. To support these expensive queries with high performance, we begin our study by analyzing the complexity of pattern queries, with a focus on the fundamental understanding of which features make pattern queries more expressive and at the same more computationally expensive. This analysis allows us to identify performance bottlenecks in processing those expensive queries, and provides key insights for us to develop a series of optimizations to mitigate those bottlenecks. Microbenchmark results show superior performance of our system for expensive pattern queries while most state-of-the-art systems suffer from poor performance. A thorough case study on Hadoop cluster monitoring further demonstrates the efficiency and effectiveness of our proposed techniques.</div><p></li>
 <li>John Meehan*, Brown University; Nesime Tatbul, Intel Labs and MIT; Stan Zdonik, Brown; Hawk Wang, MIT; Andrew Pavlo, CMU; Michael Stonebraker, MIT; Sam Madden, MIT; Ugur Cetintemel, Brown; Tim Kraska, Brown<br/><strong>S-Store: A Streaming OLTP System for Big Velocity Applications </strong><a href="javascript:toggle(36)"><br/>Click to show abstract. </a> <div id="36" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">We present S-Store, a hybrid data management system that seamlessly integrates low-latency stream processing and high-integrity transaction processing. While modern streaming databases have become very efficient at processing large quantities of data at low latency, they have major limitations when it comes to data integrity and consistency. S-Store seeks to solve this problem by using a modern, main-memory OLTP system as the foundation of a data stream management system. We define what it means to provide transactional consistency on a continuously changing dataset, and prove that our prototype performs favorably compared to its OLTP predecessor on a variety of streaming workloads.</div><p></li>
 <li>Karim Ibrahim*, WPI; Mohamed Eltabakh, WPI<br/><strong>PrefNotes: A Framework for Personalized Annotation Management in Relational Databases </strong><a href="javascript:toggle(6)"><br/>Click to show abstract. </a> <div id="6" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">Annotations play a key role in understanding and describing the data, and annotation management has become an integral component in most emerging applications such as scientific databases. Scientists need to exchange not only data but also their thoughts, comments and annotations on the data as well. Annotations represent comments, Lineage of data, description and much more. Therefore, several annotation management techniques have been proposed to efficiently and abstractly handle the annotations. However, with the increasing scale of collaboration and the extensive use of annotations among users and scientists, the number and size of the annotations may far exceed the size of the original data itself. Therefore, among the so many existing annotations, different users may have different preferences and only small number of annotations can be of interest to each user based on his (her) preferences. However, current annotation management techniques report all annotations to users without taking into account their preferences. In this poster, we propose PrefNotes , a framework for personalized annotation propagation in relational databases. PrefNotes captures users' preferences and profiles and personalizes the annotation propagation at query time by reporting the most relevant annotations (per tuple) for each user. PrefNotes supports static and dynamic profiles for each user. We propose three different kinds of filtering skyline, Top K and clustering. There are two variants of Top K operators namely fixed and proportional operators that differ in their cost model and accuracy. PrefNotes is implemented inside PostgreSQL.</div><p></li>
 <li>Lei Cao*, WPI; Qingyang Wang, WPI; Elke Rundensteiner, WPI<br/><strong>Scalable Distance-Based Outlier Detection over High-Volume Data Streams </strong><a href="javascript:toggle(45)"><br/>Click to show abstract. </a> <div id="45" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">The discovery of distance-based outliers from huge volumes of streaming data is critical for modern applications ranging from credit card fraud detection to moving object monitoring. In this work, we propose the first general framework to handle the three major classes of distance-based outliers in streaming environments, including the traditional distance-threshold based and the nearest-neighbor-based definitions. Our LEAP framework encompasses two general optimization principles applicable across all three outlier types. First, our &#96;&#96;minimal probing'' principle uses a lightweight \textit{probing} operation to gather minimal yet sufficient evidence for outlier detection. This principle overturns the state-of-the-art methodology that requires routinely conducting expensive complete neighborhood searches to identify outliers. Second, our &#96;&#96;lifespan-aware prioritization'' principle leverages the temporal relationships among stream data points to prioritize the processing order among them during the probing process. Guided by these two principles, we design an outlier detection strategy which is proven to be optimal in CPU costs needed to determine the outlier status of any data point during its entire life. Our comprehensive experimental studies, using both synthetic as well as real streaming data, demonstrate that our methods are 3 orders of magnitude faster than state-of-the-art methods for a rich diversity of scenarios tested yet scale to high dimensional streaming data.</div><p></li>
 <li>Leilani Battle*, MIT<br/><strong>Automatic Visualization Selection </strong><a href="javascript:toggle(32)"><br/>Click to show abstract. </a> <div id="32" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">Current visualization systems force users to decide in advance how to visualize their data, regardless of the user's familiarity with the given data set. Users have to make many key decisions that directly influence the quality of the visualization, such as the visualization type and x- and y-axes, with limited knowledge of how to best visualize the underlying data. For visualization systems where the intended use is for visual exploration of data, the user may not have prior experience visualizing the data set they want to explore. Thus a lack of experience with visualizing the underlying data makes it nearly impossible for users to make good visualization choices in advance, and can result in many iterations of trial and error as users search for a suitable way to visualize the data. To help users quickly make better visualization-specific choices early in the exploration process, we are designing a predictive model for identifying the most relevant visualization types and potential axes for a given data set. The predictive model will help visualization systems decide what visualization types, such as scatterplots or heat maps, and what columns of the given data set are most appropriate or interesting for visualization.</div><p></li>
 <li>Maryam Hasan*, WPI; Elke Rundensteiner, WPI; Emmanuel Agu , WPI<br/><strong>Inferring the Emotion of Twitter Users from their Tweets </strong><a href="javascript:toggle(30)"><br/>Click to show abstract. </a> <div id="30" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">Social media and microblogging tools such as twitter are increasingly used by individuals to broadcast their thoughts, opinions and emotions. Understanding the emotions of individuals will help us better interpret their behaviours and can provide useful applications. For example it helps healthcare agencies and physiologists to track behaviours and respond to people in distress. It can also be used by advertising agencies and help them what to sell. In this paper we propose a new approach for automatically classifying the twitter messages based on the emotional states of their authors. To model the emotional states of users, we utilize a popular representation of human moods, known as the Circumplex model that characterizes affective experience along two dimensions: valence and arousal. We apply machine learning algorithms for classifying tweets. Our approach is to utilize Twitter messages with hash-tags as training data which are forms of noisy labels. This type of training data is freely available and can be obtained automatically using the twitter API. Using this automatically labeled data enables us to train supervised classifiers for multi-class emotions detection in Twitter messages on potentially huge data sets with no manual effort. Further, we applied dimension reduction methods to tackle the problem of sparse and high dimensional feature vectors of twitter messages. We show that using the machine learning algorithms for classifying the moods of Twitter messages, we were able to achieve above 87% accuracy when trained with hash-tag data. </div><p></li>
 <li>Matteo Brucato*, UMass Amherst; Rahul Ramakrishna, UMass Amherst; Alexandra Meliou, UMass; Azza Abouzied, New York University, Abu Dhabi, UAE<br/><strong>PackageBuilder: From Tuples to Packages </strong><a href="javascript:toggle(4)"><br/>Click to show abstract. </a> <div id="4" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">We present PackageBuilder, a system that extends query engines to support package generation. A package is a collection of tuples with certain global properties defined on the collection as a whole. In contrast to traditional query answers, where each answer tuple needs to satisfy the query predicate constraints, each answer package needs to satisfy global constraints on the collection of tuples: e.g., a package of recipes that collectively do not exceed 2,200 calories. PackageBuilder introduces simple extensions to the SQL language to support package-level predicates, and includes a simple interface that allows users to load datasets and interactively specify package queries. Our system allows users to interactively navigate through the result packages, and to provide feedback by fixing tuples within a package. PackageBuilder automatically processes this feedback to refine the package queries, and generate new sets of results.</div><p></li>
 <li>Matteo Riondato*, Brown University Department of Computer Science; Evgenios Kornaropoulos, Brown University Department of Computer Science<br/><strong>Fast Approximation of Betweenness Centrality through Sampling </strong><a href="javascript:toggle(28)"><br/>Click to show abstract. </a> <div id="28" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">Betweenness centrality is a fundamental measure in social network analysis, expressing the importance or influence of individual vertices in a network in terms of the fraction of shortest paths that pass through them. Exact computation in large networks is prohibitively expensive and fast approximation algorithms are required in these cases. We present two efficient randomized algorithms for betweenness estimation. The algorithms are based on random sampling of shortest paths and offer probabilistic guarantees on the quality of the approximation. The first algorithm estimates the betweenness of all vertices: all approximated values are within an ad- ditive factor &#949; from the real values, with probability at least 1 &#8722; &#948;. The second algorithm focuses on the top-K vertices with highest betweenness and approximate their betweenness within a multiplicative factor &#949;, with probability at least 1 &#8722; &#948;. This is the first algorithm that can compute such approximation for the top-K vertices. We use results from the VC-dimension theory to develop bounds to the sample size needed to achieve the desired approximations. By proving upper and lower bounds to the VC-dimension of a range set associated with the problem at hand, we obtain a sample size that is independent from the number of vertices in the network and only depends on a characteristic quantity that we call the vertex-diameter, that is the maximum number of vertices in a shortest path. In some cases, the sample size is completely independent from any property of the graph. The extensive experimental evaluation that we performed using real and artificial networks shows that our algorithms are significantly faster and much more scalable as the number of vertices in the network grows than previously pre- sented algorithms with similar approximation guarantees.</div><p></li>
 <li>Michael Gubanov*, MIT; Michael Stonebraker, MIT<br/><strong>Bootstraping Synonym Resolution at Web-scale </strong><a href="javascript:toggle(27)"><br/>Click to show abstract. </a> <div id="27" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">Big data heterogeneity makes the long-standing entity-resolution/synonym detection class of problems pressing again. It is no longer feasible to rely on semi-automatic solutions. Here we present our web-scale synonymous entity classification approach that can be used to efficiently automate many steps in the data integration pipeline ranging from metadata to more sophisticated instance-level matching and mapping algorithms. Having access to an ultra-large scale dataset, we bootstrap a scalable machine learning classifier and demonstrate its high accuracy on several entity types. We plan to use and further these initial results to reach our long-term goal to efficiently detect and leverage structure in ultra-large heterogeneous datasets. </div><p></li>
 <li>Neha Narula*, MIT CSAIL<br/><strong>Serializable Transactions Across A Partitioned Cache </strong><a href="javascript:toggle(44)"><br/>Click to show abstract. </a> <div id="44" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">Application developers use in-memory caches like memcached to reduce load on a centralized database, often at the expense of consistency. Techniques for executing transactions over partitioned in-memory databases presume that all data being addressed is of equal importance; we suggest techniques to exploit the fact that some data is \emph{base data} and other data is \emph{derived data} stored in an in-memory cache. The main difference between derived data and base data is that derived data can always be deterministically recomputed from base data, so partitions holding derived data can fail without loss of durability, but with a potentially big effect on overall read performance during recovery. We present the design of a system for efficiently executing serializable transactions over a database and set of partitioned cache servers. The system divides transactions and uses multi-version concurrency control to avoid holding locks in the cache so reads are never blocked, and we show that we can safely asynchronously update caching servers. In addition, we sketch out a design for fast cache recovery. </div><p></li>
 <li>Olga Poppe*, WPI; Elke Rundensteiner, WPI<br/><strong>HIT: Workflow-based Emergency Management Using Prioritized Run Execution Strategy </strong><a href="javascript:toggle(15)"><br/>Click to show abstract. </a> <div id="15" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">We propose a powerful yet human-comprehensive model, called HIT, for specification of complex workflow-determined stream processing applications. The HIT model visually captures involved workflow in the form of the Hierarchical Instantiating Timed automaton and allows to formulate queries within its states. In practice, huge number of concurrent runs of the HIT automaton must be processed against high-rate streams. To allow for near real-time responsiveness despite such workloads, the HIT executor employs a prioritized run-driven stream processing strategy. Furthermore, identical runs are efficiently recognized and merged to minimize repeated computations. We demonstrate the effectiveness of the HIT technology using the emergency management scenario from application specification to its optimized processing.</div><p></li>
 <li>Ori Herrnstadt*, FoundationDB<br/><strong>Merging SQL and NoSQL </strong><a href="javascript:toggle(14)"><br/>Click to show abstract. </a> <div id="14" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">As distributed NoSQL databases are maturing and gaining more traction, there is an increasing need for SQL interfaces to ease their adoption. In this talk I'll discuss the challenges to building efficient SQL interfaces targeted at high throughput OLTP applications. I&rsquo;ll show how expanded data model semantics and parallelism can help mitigate latency-bound performance, and argue that some guarantees must be provided by the underlying NoSQL system itself. Both Google F1 and FoundationDB SQL Layer follow this approach. Much of the content in this talk will be related to lessons learned while building and supporting the SQL Layer on FoundationDB, but equally apply to other NoSQL systems.</div><p></li>
 <li>Padmashree Ravindra*, North Carolina State Universit; Kemafor Anyanwu, North Carolina State University<br/><strong>Optimizing Flexible Graph Pattern Queries for Big Data </strong><a href="javascript:toggle(31)"><br/>Click to show abstract. </a> <div id="31" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">Big Data platforms such as Apache Hive and Pig are being extensively used for scalable processing of semi-structured data. However, there has been little focus on optimizing &#96;unbound-property' graph pattern queries that enable flexible exploration of semi-structured datasets with unknown relationships. Relational-style processing of such queries in systems such as Hive and Pig, results in redundant information in intermediate results due to the repetition of adjoining bound (fixed) properties. Such redundancy negatively impacts the disk I/O, network transfer costs, and the required disk space while processing graph pattern query workloads on MapReduce-based systems. This work proposes packing and lazy unpacking strategies to minimize the redundancy in intermediate results while processing unbound-property queries. In addition to keeping the results compact, this work evaluates flexible graph pattern queries using the Nested TripleGroup Data Model and Algebra (NTGA) that enables shorter MapReduce execution workflows. Experimental results demonstrate the benefit of this work over processing unbound-property queries using relational-style systems such as Apache Pig and Hive.</div><p></li>
 <li>Patrick Haren*, Cigna &amp; Bridgeport U<br/><strong>Declarative models for information production on NewSQL architectures </strong><a href="javascript:toggle(40)"><br/>Click to show abstract. </a> <div id="40" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">For NewSQL systems, investigate options for alternatives to the data transformation patterns of ETL/ELT. As updates occur to (typically normalized) transactional databases, what could replace staid ETL logic in order to immediately produce alternatively organized views or summaries of this data? The research will pick some subject areas &ndash; e.g. clinical/medical settings, financial organizations, fraud detection, situation management and science/research. For these areas, could we represent the bubbling-up of information resulting from large datasets, via models? Potential model approaches are: - knowledge inference rules [given data conditions, determine insights] - probability decision trees [Expected Values, most likely values, &#8230;] - combination and stacking of these For simplicity, we will pick a single DBMS, such as H-Store, to evaluate the practicality of such model implementations. Depending on the models, additional research and development could look at how to scale &ndash; such as with map/reduce workloads, other partitioning schemes and/or new algorithms. We could also look at approaches to consistency, without locking.</div><p></li>
 <li>Samyuktha Sridharan*, UMass Lowell; Chunyao Song, UMass Lowell; Tingjian Ge, University of Massachusetts, Lowell; Rachelle Damle, UMass Memorial Medical Center; Karim Alavi, UMass Memorial Medical Center<br/><strong>Predicting and Reducing Hospital Readmissions </strong><a href="javascript:toggle(26)"><br/>Click to show abstract. </a> <div id="26" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">There has been an increasing interest in the research for predicting patient readmission. A recent research showed that almost 20% of Medicare patients were readmitted within 30 days of discharge. Reducing the national readmission rates could help Medicare save $1.9 billion annually. The efforts to decrease healthcare readmission costs, improve healthcare quality coupled with the large volume of available data has resulted in more focus on reducing patient readmissions. For this purpose, several models have been developed using healthcare data. This research first focuses on studying the different prediction models developed for hospital readmission data so far. Methods such as Poisson Regression Model and Boosted Decision Trees have been used to identify and measure potentially avoidable readmissions. Hierarchical Generalized Linear Models (HGLM) such as Logistic Regression models have been used to predict readmission of all-cause 30 day readmissions for several diseases. While the Poisson Regression Model and HGLM are not able to handle missing data, the computational time using Boosted Decision Trees is very high. In this research, we propose a new prediction method for hospital readmissions using our newly devised quantitative association rule mining algorithm. We use a real application and datasets to verify our approach. </div><p></li>
 <li>Stephen Tu*, MIT<br/><strong>Differentially private stochastic gradient descent for regularized empirical risk minimization </strong><a href="javascript:toggle(20)"><br/>Click to show abstract. </a> <div id="20" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">With more sensitive information being data-mined by learning algorithms, it is important that practitioners understand the privacy and utility trade-offs from these statistical models. A recent definition of privacy, known as $\epsilon$-differential privacy from the cryptography community (\cite{dwork06}), provides a rigorous framework to analyze various learning algorithms. Previous work (e.g. \cite{chaudhuri11}, \cite{rubinstein09}, \cite{kifer12}) has focused on providing differentially private algorithms in the context of regularized empirical risk minimization (ERM) for convex loss functions. These algorithms require one to compute the \textit{exact} minimizer for privacy guarantees to hold. In practice, however, one often computes an \textit{approximation} to the exact solution via gradient methods such as stochastic gradient descent (SGD). While great from a computational perspective, it is unclear whether or not the privacy guarantees still hold in this setting. In this paper, we focus on SGD for regularized ERM learning, and show how to release a approximate model in a differentially private manner. Our method works by perturbing the final result; as we run more iterations, we recover the noise bound from \cite{chaudhuri11} for exact minimization. Empirically, we demonstrate that this happens with relative few number of iterations.</div><p></li>
 <li>Tingjian Ge, University of Massachusetts, Lowell; Chunyao Song*, UMASS Lowell; Zheng Li, <br/><strong>Aroma: A New Data Protection Method with Good Privacy and Utility Properties </strong><a href="javascript:toggle(12)"><br/>Click to show abstract. </a> <div id="12" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">Releasing sensitive information while preserving individuals' privacy is crucial for the burgeoning data. We propose a new local data perturbation method called Aroma to address this problem. To show that Aroma is sound in its privacy protection, we devise a realistic privacy game, called the exposure test. We show that the alpha-beta algorithm, a previously proposed method that is most closely related to Aroma, performs poorly under the exposure test and fails to provide sufficient privacy in practice. Moreover, epsilon-differential privacy provides strong privacy guarantees and we show that any data protection method that satisfies epsilon-differential privacy will succeed in the test. We show that Aroma offers strong privacy protection by deriving parameters that let Aroma satisfy the epsilon-differential privacy. This Aroma parameter also serves as a tradeoff between privacy and utility. In our evaluation, we show that Aroma estimator has significantly smaller errors than previous state-of-the-art data protection methods such as the alpha-beta and the FRAPP algorithms.</div><p></li>
 <li>Tobias M&uuml;hlbauer*, Technische Universit&auml;t M&uuml;nchen; Florian Funke, ; Viktor Leis, ; Henrik M&uuml;he, ; Wolf R&ouml;diger, ; Alfons Kemper, ; Thomas Neumann, <br/><strong>Poster: The HyPer System </strong><a href="javascript:toggle(25)"><br/>Click to show abstract. </a> <div id="25" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">Ever increasing main memory capacities and processors with multiple cores have fostered the development of database systems that process and store data solely in main memory. This poster presents HyPer, a high-performance hybrid OLTP&amp;OLAP main memory database system. Unlike other main memory database systems, HyPer aims at providing highest performance for both, OLTP AND OLAP workloads on brawny AND wimpy systems. OLAP query processing is separated from mission-critical OLTP transaction processing using an efficient virtual memory (VM) snapshotting mechanism. Platform-independent high-performance OLTP and OLAP is achieved by efficiently compiling transactions and queries into efficient target machine code. Even though the SQL-92 standard, a PL/SQL-like scripting language, and ACID-compliant transactions are supported, HyPer has a memory footprint of just a few megabytes. In particular, this poster highlights recent research efforts in the HyPer project, including (i) the adaptive radix tree (ART), (ii) using Intel's recent hardware transactional memory (HTM) features for transaction processing, (iii) tentative execution of long-running transactions, (iv) compaction of memory-resident data, (v) efficient bulk loading of flat files at the wire speed of SSDs and 10 GbE adapters, (vi) the development of a locality-sensitive data-shuffling scheme, and (vii) a scaled-out version of the HyPer system that allows elastic OLAP throughput on transactional data.</div><p></li>
 <li>Xiangyao Yu*, MIT; George Bezerra, MIT; Michael Stonebraker, MIT; Srinivas Devadas, MIT; Andrew Pavlo, CMU<br/><strong>Concurrency Control in the Many-core Era: Scalability and Limitations </strong><a href="javascript:toggle(23)"><br/>Click to show abstract. </a> <div id="23" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic"></div><p></li>
 <li>Xiao Qin*, Worcester Polytechnic Institute; Ramoza Ahsan, Worcester Polytechnic Institute; Xika Lin, Worcester Polytechnic Institut; Elke Rundensteiner, WPI; Matt Ward, Worcester Polytechnic Institute<br/><strong>IncPARAS: An Incremental Parameter Space Load Pipeline for Interactive Association Mining </strong><a href="javascript:toggle(46)"><br/>Click to show abstract. </a> <div id="46" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">Association rule mining is known to be computationally intensive, yet real-time decision-making applications are increasingly intolerant to delays. Our previous model PARAS, a parameter space framework for online association mining, enables efficient rule mining by compactly maintaining the final rulesets and provides efficient query-time redundancy resolution. As many association mining models, PARAS was designed for static data. However, updates to transaction database could invalidate existing rules or introduce new rules, rendering PARAS unfit for dynamic database. Meanwhile, it might lead to dramatic changes of corresponding redundancy relationships. Our proposed model IncPARAS, an incremental parameter space load pipeline for interactive association mining, utilizes the previous mining result to update the rulesets and the redundancy relationships. In particular, IncPARAS features two innovative techniques. First, IncPARAS develops a novel incremental frequent itemsets mining technique. Second, IncPARAS designs a compact data structure to efficiently maintain the complex redundancy relationships. Overall, IncPARAS achieves favorable speed-up comparing to mining association in updated database from scratch.</div><p></li>
 <li>Xika Lin*, Worcester Polytechnic Institut; Elke Rundensteiner, WPI; Matthew Ward, <br/><strong>EvoCommunity: Querying and Tracking Evolutionary Community </strong><a href="javascript:toggle(22)"><br/>Click to show abstract. </a> <div id="22" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">Data stream, in which data evolves continuously, exist in a wide variety of applications, and identifying communities and their involving evolution from data stream is an important and challenging task. In this paper, we introduce our EvoCommunity system that supports interactive mining of community evolution pattern within live data streams. Evocommunity is equipped with both computational pattern mining and visualization techniques, which allow it to not only efficiently discover and manage patterns but also effectively convey the mining results to human analysts through visual displays. EvoCommunity is composed of four major components. First, as foundation of EvoCommunity, we develop an evolution model of communities in data streams that captures the complete spectrum of community evolution types across streaming windows. Second, to equip EvoCommunity with the capability of efficiently tracking community, we design a novel algorithm to push the evolution tracking process into the underlying community detection process. Third, to provide users with rich insights into evolving community, we introduce a broad range of query types and efficient query process. Fourth, to facilitate users' understanding, we provide a rich set of visualization and interaction techniques to allow users to explore the pattern in real-time.</div><p></li>
 <li>Yuan Mei*, MIT; Sam Madden, MIT<br/><strong>The SPRAWL Distributed Stream Processing System </strong><a href="javascript:toggle(29)"><br/>Click to show abstract. </a> <div id="29" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">Many large financial, news, and social media companies stream large quantities of data to customers, either through the public internet or on their own internal networks. These customers often depend on that data being delivered in a timely and efficient manner. In addition, many customers subscribe to the same or similar data products (e.g., particular types of financial feeds, or feeds of specific social media users). A naive implementation of a data dissemination network like this will cause redundant data to be delivered repeatedly, wasting bandwidth, increasing network delays, and driving up costs. In this paper, we present SPRAWL, a stream processing system designed to address the wide-area data processing and dissemination problem. It allows users to pose SQL-like queries over distributed data streams and execute them efficiently. SPRAWL provides two key functions. First, it is able to generate a shared and distributed multi-query plan that transmits records through the network just once, overlapping the computation of streaming operators that op- erate on the same subset of data. Second, it is able to compute an in-network placement of operators. This placement minimizes the overall plan cost for a class of cost functions that consist of a weighted sum of plan CPU usage, bandwidth consumption and latency. The placement is optimal when resource constrains are not violated, and is near optimal when constraints are reached. Overall, we show that this can increase throughput by up to a factor of 5 and reduce dollar costs by a factor of 6 on a financial data feed processing benchmark.</div><p></li>
 <li>YUE LU*, WPI<br/><strong>CloudNotes: Annotation Management in Cloud-&shy;&#8208;Based Platforms </strong><a href="javascript:toggle(5)"><br/>Click to show abstract. </a> <div id="5" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">We present an annotation management system for cloud-based platforms, which is called &ldquo;CloudNotes&rdquo;. CloudNotes enables the annotation management feature in the scalable Hadoop and MapRedue platforms. In CloudNotes system, every piece of data may have one or more annotations associate with it, and these annotations will be propagated when the data is being transformed through the MapReduce jobs. Such an annotation management system is important for understanding the provenance and quality of data, especially in applications that deal with integration of scientific and biological data at unprecedented scale and complexity. We propose several extensions to the Hadoop platform that allow end-users to add and retrieve annotations seamlessly. Annotations in CloudNotes will be generated, propagated and managed in a distributed manner. We address several challenges that include attaching annotations to data at various granularities in Hadoop, annotating data in flat files with no known schema until query time, and creating and storing the annotations is a distributed fashion. We also present new storage mechanisms and novel indexing techniques that enable adding the annotations in small increments although Hadoop&rsquo;s file system is optimized for large batch processing.</div><p></li>
 <li>Yue Wang*, UMass Amherst; Gerome Miklau, UMass Amherst; Prashant Shenoy, UMass Amherst<br/><strong>MR-ADVI$OR: Managing Cost and Time for Map-Reduce in the Cloud </strong><a href="javascript:toggle(19)"><br/>Click to show abstract. </a> <div id="19" style = 'display: None;'> <p style = "color:black; font-size:100%; font-style:italic">We describe the MR-ADVI$OR system for tuning and optimization of MapReduce algorithms executed using on-demand cloud resources. In the cloud, optimization objectives are complex because financial constraints must be balanced with efficiency goals. MR-ADVI$OR estimates, for a MapReduce algorithm implemented as a Hadoop program, the relationship between total runtime and financial cost for different cloud configurations using a novel combination of analytic modeling and log-based analysis. MR-ADVI$OR then uses this estimate, in conjunction with a description of a user's preferences, to predict a desirable configuration. The proposed demonstration allows participants to use MR-ADVI$OR in a simulated workflow and to assess its prediction accuracy and the value of the predicted configuration.</div><p></li>

</ul>
</td></tr>
</table>
</body>
</html>
