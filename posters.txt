Auto Query Steering for Interactive Data Exploration Applications	"Interactive Data Exploration (IDE) is a key ingredient of a diverse set of discovery-oriented applications, including ones from scientific computing and evidence-based medicine. In these applications, a common pattern is observed: while a content expert can judge whether a given data is of interest or not he is unaware of the exact predicates that would be used to formulate a query to collect all relevant data. Specifically, data discovery is a highly ad hoc interactive process where one executes numerous exploration queries using varying predicates aiming to balance the trade-off between collecting all relevant information and reducing the size of returned data. As data volumes continue to grow, IDE can become an extremely labor and resource intensive process. Therefore, there is a strong need to support these human-in-the-loop applications by assisting their navigation in their data set to find interesting objects.  We propose an automated data exploration system that employs an iterative query steering framework to steer the user towards interesting data areas. Our approach leverages relevance feedback on database samples to model the user interest and strategically collect more samples to refine the model, aiming to efficiently identify all relevant to the user objects. Our system integrates machine learning and data management techniques to provide effective data exploration results (matching the user interest well) as well as high interactive performance. Specifically, our system leverages the classification properties of decision trees to identify in real time new training sample sets that will quickly improve our understanding of the user interests and eventually infer a query that retrieves data relevant to his interests.  In this poster we will discuss the challenges of supporting IDE applications and describe our automatic data exploration framework. We will also present preliminary experimental results that demonstrate the effectiveness of our approach. "	"Kyriaki Dimitriadou, Brandeis University; Olga Papaemmanouil, Brandeis University; Yanlei Diao, UMass Amherst"
Data Curation at Scale: The Data Tamer System 	"Data curation is the act of discovering a data source(s) of interest, cleaning and transforming the new data, semantically integrating it with other local data sources, and deduplicating the resulting composite. There has been much research on the various components of curation (especially data integration and deduplication). However, there has been little work on collecting all of the curation components into an integrated end-to-end system.    In addition, most of the previous work will not scale to the sizes of problems that we are finding in the field. For example, one web aggregator requires the curation of 80,000 URLs and a second biotech company has the problem of curating 8000 spreadsheets. At this scale, data curation cannot be a manual (human) effort, but must entail machine learning approaches with a human assist only when necessary.    This poster describes Data Tamer, an end-to-end curation system we have built at M.I.T. Brandeis, and Qatar Computing Research Institute (QCRI). It expects as input a sequence of data sources to add to a composite being constructed over time. A new source is subjected to machine learning algorithms to perform attribute identification, grouping of attributes into tables, transformation of incoming data and deduplication. When necessary, a human can be asked for guidance.    We have run Data Tamer on three real world enterprise curation problems, and it has been shown to lower curation cost by about 90%, relative to the currently deployed production  software."	"Alexander Pagan, MIT"
Data Virtualization	"Data virtualization is an agile data integration method that simplifies information access.   Compared to other integration approaches such as data consolidation via data warehouses and ETL, or data replication via ESBs and FTP, data virtualization queries data from diverse sources on demand without requiring extra copies.   Large organizations including ten of the top 20 banks, seven of the top ten pharmaceuticals, four of the top five energy companies, leading telecommunications, media and technology companies as well as government agencies, use the Composite Data Virtualization Platform to simplify their information access.  These organizations have made Composite Software their data virtualization gold standard, selecting Composite over other data virtualization providers for our in-depth understanding of their business and technology challenges as well as our proven products and high-caliber services.  "	"Alexander Rivilis, Composite Software"
Detecting Anomalies In Big Data Streams	"The discovery of anomalies from huge volumes of streaming data has been recognized as critical for modern applications ranging from credit card fraud detection, financial market analysis to moving object monitoring. In this work, we propose a new optimization framework to detect anomalies in streaming environments. Our framework encompasses two optimization principles. The first principle overturns the state-of-the-art methodology that relies on expensive neighborhood search to identify anomalies. The second principle leverages the temporal relationships among stream data points. Under the guidance of these two principles, we then design the anomaly detection strategy which is confirmed by our extensive experiments to be three orders magnitude faster than the state-of-the-art."	"Lei Cao, Worcester Polytechnic Institut; Elke Rundensteiner, WPI; Jiayuan Wang, ; Qingyang Wang, Worcester Polytechnic Institute"
Dynamic Reduction of Query Result Sets for Interactive Visualization	"Modern database management systems (DBMS) have been designed to efficiently  store, manage and perform computations on massive amounts of data. In contrast,  many existing visualization systems do not scale seamlessly from small data  sets to enormous ones. We have designed a three-tiered visualization system  called ScalaR to deal with this issue. ScalaR dynamically performs resolution reduction when the  expected result of a DBMS query is too large to be effectively rendered on  existing screen real estate. Instead of running the original query, ScalaR  inserts aggregation, sampling or filtering operations to reduce the size of the  result. This poster presents the design and implementation of ScalaR, and  shows results for an example use case of ScalaR, displaying satellite imagery data  stored in SciDB as the back-end DBMS.  "	"Leilani Battle, MIT"
Evidence-based Iterative Multi-hypotheses Testing	"Recently, CERN physicists found traces of the 'god particle' Higgs Boson, believed to be vital in the creation of the universe. However, they stopped well short of declaring it a ground breaking discovery because of insufficient data for establishing statistical significance. Such scientific analysis uses hypothesis testing for statistical significance. Traditionally it is carried out on data and is highly dependent on the domain expertise of the analysts. All tasks starting from formulation of hypothesis and experimental design for testing it, to selection of suitable statistical tests and interpretation of results are performed by the analyst. Statistical tests are the only automated component of the entire hypothesis testing process. Given the high volumes of data being generated by such large scale experiments, it has become virtually impossible for analysts to manually inspect all the data to comprehensively test a hypothesis.    The motivation for my research is to "help scientists perform research and validate their discoveries better''. I design novel computation techniques for automating the hypothesis testing process. In contrast to existing works that must use huge amounts of raw experimental data, I devise hypothesis testing techniques on data mining results (nuggets). Nuggets, such as association rules or clusters, represent prominent patterns valid within the data and the input to the hypothesis tests is greatly reduced. I first define interestingness measures for single hypothesis testing using nuggets. I discover several shortcomings of the single hypothesis testing techniques. I propose a novel iterative multi-hypotheses testing approach that overcomes those shortcomings."	"Abhishek Mukherji, WPI; Elke Rundensteiner, WPI; Matthew Ward, WPI"
Fusing Probabilistic Data with Correlations	"Recent trends in knowledge-driven tasks have increased interest in extracting structured relations from web documents. Such extraction is inherently hard, and automatic extraction systems invariably produce noisy data. Cleaning the data is not straightforward: extracted data is typically uncertain, and different extraction systems often provide duplicate or conflicting results. Our framework automatically integrates results from multiple extraction systems, targeting three main challenges: (a) Users often see extraction methods as black boxes, without access to the methods' internal details other than high-level summaries of the quality (e.g., precision and recall) of each method. (b) The output of the extraction systems is probabilistic, due to uncertainty in the source data, and the probabilistic nature of the extraction process. (c) Extraction methods may have unknown correlations, but since access to their details is limited, these correlations are hard to infer. Challenges (b) and (c) set our framework apart from existing techniques.  We evaluate our approach against a real-world dataset extracted from seven web sources in the restaurant domain, as well as against synthetic data. We compare our algorithms to state-of-the-art techniques and present results of the precision, accuracy, and robustness of our approach."	"Ravali Pochampally, UMass-Amherst; Anish Das Sarma, ; Xin Luna Dong, AT & T Research; Alexandra Meliou, UMass Amherst; Divesh   Srivastava, AT & T Labs-Research"
GenDB: a Benchmark for Big Data Genomics	"Large volumes of genomic data is currently being generated through techniques such as next-gen sequencing, microarrays etc., and this data will keep growing as genomic technologies become cheaper. Traditional DB solutions which focus on operations such as select, project, join etc. are inadequate for genomic workloads which are typically heavy in linear algebra and statistics. By collaborating with Novartis and the Broad Institute, we have developed a benchmark for queries used most often in genomics. The benchmark queries require a combination of large scale data management, statistics and linear algebra. This poster will present the genomic benchmark and results of running this benchmark on various systems including R, Postgres, MADlib and SciDB."	"Manasi Vartak, MIT"
Generating Private Synthetic Databases for Untrusted System Evaluation	"Evaluating the performance of database systems is crucial when database vendors or researchers are developing new technologies. But such evaluation tasks rely heavily on actual data and query workloads that are often unavailable to researchers due to privacy restrictions. To overcome this barrier, we propose a framework for the release of a synthetic database which accurately models selected perfor- mance properties of the original database. We improve on prior work on synthetic database generation by providing a formal, rigorous guarantee of privacy. Accuracy is achieved by generating synthetic data using a carefully selected set of statistical properties of the original data which balance privacy loss with relevance to the given query workload."	"Wentian Lu, UMass Amherst; Gerome Miklau, University of Massachusetts Amherst; Vani Gupta, "
Google Spanner and Akiban: Opposite Ends of the Same Spectrum  	"Google Spanner was mainly built because the most important system at Google, Google AdWords, had accumulated Terabytes of data on MySQL. It is a well known fact that Google has always trusted and relied on a relational database to power the system that generated more than 90% of Google's revenue. When the amount of data grew, Google developed something called "schematized, semi-relational tables" - or something Akiban calls Table Groups.    This is a big deal: Google and Akiban are using the same approach to storage and we have a similar data model. The difference? We're attacking the same problem from opposite ends of the spectrum. Akiban is currently focused on solid single and multi-node performance while Google is focused on global-scale, Google-sized transactions.    In this presentation, Ori Herrnstadt will present on why a forty-year old technology, Codd's relational database, is here to stay. He will talk about how the relational database has been evolving and growing with the demands of Big Data. We will focus on Table Groups: why and how they make a difference in the evolution of the relational database. A tool so powerful that Google has developed a storage system exactly like Akiban's. You will learn how Akiban, in addition to achieving efficiency by co-locating data together, revamped the way queries are executed to take advantage of the entire "object graph" available to us without having to perform any joins. While Google is focused on creating systems that can survive the unexpected destruction of multiple data centers, we're focused on reapplying some of Codd's wisdom to what we think has been established as the future direction of relational database storage."	"Rosa Hu, Akiban Technologies; Ori Herrnstadat, Akiban Technologies"
"Intelligent Metabolic Isotopomer Data Extraction, Classification, Standardization and Management"	"Nowadays; Computer Science approaches i.e. software, database, management systems etc., are powerful tools to boost research in all fields of life. Benefiting in the fields of bioinformatics and biochemistry; here they are applied to metabolic data modelling in infections as well as health care management. The scope of the presented research in this paper is limited up to the demonstration of an intelligent software application i.e. Isotopo Data Classifier (IDC), towards automatic isotopomer data extraction, classification, standardization and management using a newly proposed special purpose data classifier based on supervised machine learning principles. IDC is an integrated module of a product line application i.e. Software for Biological Experimental Data Analysis (SBEDA), capable of automatically extracting data from different formats, classifying in to the standardized data format compatible to the software application i.e. Isotopo (towards mass isotopomers distribution analysis) and managing in database as well as third party independent file based data management system. IDC is designed with software engineering principles and implemented using Microsoft Dot Net framework, tested in house and validated using metabolic experimental data."	"Zeeshan Ahmed, University of Wuerzburg; Wolfgang Eisenreich, Department of Chemistry,Technical University of Munich; Thomas Dandekar, Chair: Department of Bioinformatics, Biocenter, University of Wuerzburg"
Interactive Data Exploration Using Semantic Windows	"We introduce Semantic Windows (SW), a new query model for interactive data exploration of large data sets. SW extends SQL with constructs to express multidimensional regions of the underlying data space using shape and content predicates. This model allows a host of of useful exploratory queries that are difficult to express and optimize using standard database techniques.    SW uses a sampling-guided search strategy to explore the data and quickly identify windows of interest. To enable interactivity, SW provides online results during query execution. To control the tension between online performance and query completion time, it uses a tunable prefetching technique."	"Alexander Kalinin, Brown University; Ugur Cetintemel, Brown University; Stan Zdonik, Brown University"
MailX	"The problem with current mailing lists is that they are all-or-nothing; when one sends an email, everyone gets it. People need to feel like their email is very important before they use the list, which makes polite people reluctant to do it at all. Many not-so-polite people do reply all and send to entire list which causes inbox flooding. Many unsubscribe from the list and miss useful information. Mail-X is a mailing list manager that allows more controlled, yet effective information dissemination."	"Anant Bhardwaj, MIT; David Karger, MIT; Samuel Madden, MIT"
Massive Genomic Data Processing and Deep Analysis	"Today large sequencing centers are producing genomic data at the rate of 10 terabytes a day and require complicated processing to transform massive amounts of noisy raw data into biological information. To address these needs, we develop a system for end-to-end processing of genomic data, including alignment of short read sequences, variation discovery, and deep analysis. We also employ a range of quality control mechanisms to improve data quality and MapReduce style parallel processing for performance. We will show the details of data transformation through the workﬂow, the usefulness of end results (ready for use as testable hypotheses), the effects of our quality control mechanisms and improved algorithms, and finally performance improvement."	"Abhishek Roy, UMass Amherst; Yanlei Diao, UMass Amherst"
Modeling performance and cost of hybrid storage based LSM-trees	"LSM-trees have been widely used in key-value stores for big data of high update rate, such as Google's Big Table, Amazon's Dynamo, HBase, Cassandra, etc.  Existing LSM-tree systems are mainly based on HDD storage, using a large amount of memory to buffer active data for high performance.  However, the low locality of reference among big data makes such systems hard to scale.  Although SSD can be used as the underlying storage of LSM-trees to speed up random access, the price is still too high for big data applications.  In this work, we aim to model the performance and cost of hybrid storage based LSM-trees, which utilize DRAM as the memory for data buffer, SSD as the device for fast data access, and HDD as the storage for low cost index maintenance.  The principle of this hybrid system is based on the performance and cost ratio of these three components: the random access cost of SSD is at least one magnitude lower than HDD, while the sequential access cost of SSD is at least one magnitude higher than HDD.  Meanwhile, the maintenance cost of a LSM-tree depends on the size of its buffer, which also help to reduce the random access load on the underlying storage.  We have proposed a series of disk mirroring approaches for the hybrid architecture: synchronized, asynchronized, fully, and partially  mirroring methods, and built a mathematical model to achieve the best combination of the three storage components and mirroring methods in terms of performance-cost ratio.  Finally, we have analyzed its performance on three kinds of big data of different reference localities: uniformly random, stretched exponential, and power law.     "	"Lei Guo, Ohio State University; Xiaodong Zhang, ; Rubao Li, The Ohio State University"
NuDB: A new kind of teaching database	"This poster presents NuDB, a new kind of instructional database. Modern database design and implementation, whether it be OLTP or OLAP, is quite different from the System R design of the 70s; modern computing environments have much more CPU and memory resources than their 70s counterparts, and therefore for many workloads the dominating cost is no longer IO. Consequently, many systems have been built recently in both industry and academia which challenge many of the traditional RDBMS designs and architectures.    The author of this poster believes that the database systems used as instructional tools for undergraduate and graduate students studying database design and implementation should reflect this current trend. However, a recent survey of the database course offerings at a few well known computer science departments shows that, despite progress being made, this is not quite the case yet. To this end, this poster presents the proposed design and implementation of NuDB, which is an instructional RDBMS system designed from the ground up for modern in-memory transactional OLTP workloads. The poster will go into detail about the key design concepts that NuDB will exemplify, in addition to the performance goals set forth.  We hope to show some early performance benchmarks of NuDB's storage layer and compare its performance to  current popular open-source offerings."	"Stephen Tu, MIT"
Optimized Multiple Complex Event Processing by Shared Sub-Expression	"Complex Event Processing (CEP) has become increasingly important for tracking and monitoring anomalies and trends in event streams emitted from business processes such as supply chain management to online stores in e-commerce. These monitoring applications submit complex event queries to track sequences of events that match a given pattern. Besides sequence patterns, these applications often specify non-existence of events and also complex predicates on these patterns. Languages for pattern matching over streams are significantly richer than languages for regular expression matching. Furthermore, efficient evaluation of these pattern queries over streams requires new algorithms and optimizations: the conventional wisdom for stream query processing (i.e., using selection-join-aggregation) is inadequate. Recent work on CEP have tried to address this gap, however they have not considered a huge workload of pattern queries. In most real world applications running pattern queries there exist a huge workload of queries and large volumes of incoming data stream with fluctuating data characteristics. In this work, we will describe techniques for computation sharing in multiple CEP queries as a model of the query workload and data properties. "	"Medhabi Ray, WPI; Elke Rundensteiner, WPI"
Oxi: A Framework for Composing Relational and NoSQL Datastores	"In recent years, NoSQL solutions for data storage have grown in popularity for several reasons, including relaxed-schema design and the promise of easy scalability. Unfortunately, many of these solutions come with less-than-powerful query mechanisms, as well as relaxed transaction, durability and recovery guarantees. In this paper, we present Oxi, an open-source toolkit for composing scalable key-value storage with in-memory data structure storage, as well as range-based and fulltext secondary indexes. We examine a series of common patterns in the context of the framework that we have optimized for real-world web and mobile application development, including materialized aggregations, nested objects, fast membership queries and cursor traversal. We place particular importance on asymptotic cost of operations, achieving fixed-cost operations in many critical areas of application design."	"Sunny Gleason, SunnyCloud"
PARAS: A Parameter Space Framework for Online Association Mining.	"Association rule mining is known to be computationally intensive, yet real-time decision-making  applications are increasingly intolerant to delays. To enable online association rule mining, existing techniques thus prestore intermediate results, namely,  itemsets in an itemset-based index. However, given  particular input parameter values such as minsupp and  minconf, the actual rule generation must still be performed at query-time. The response time can be unacceptably long for interactive mining, especially when rule redundancy resolution is required as part of rule generation. To tackle this shortcoming, we introduce the parameter space model, called PARAS. PARAS enables efficient rule mining by compactly maintaining the final rulesets instead of just  storing intermediate itemsets. The PARAS model is based on the notion of stable region abstractions that form the coarse granularity ruleset space. Based on new insights on the redundancy relationships among rules, PARAS establishes a surprisingly compact representation of complex  redundancy relationships while enabling efficient redundancy resolution at query-time. Besides the classical rule mining requests, the PARAS model supports three novel classes of exploratory queries. Using the proposed PSpace index, these exploratory query classes can all be answered with near real-time responsiveness. Our experimental evaluation using  several benchmark datasets demonstrates that PARAS achieves 2 to 5 orders of magnitude improvement over commonly used techniques in online association rule mining. This work is accepted for publication in VLDB 2013."	"Xika Lin, WPI; Abhishek Mukherji, WPI; Elke Rundensteiner, WPI; Carolina Ruiz, WPI; Matthew Ward, WPI"
Pricing Aggregate Queries in a Data Marketplace	"Personal data has value to both its owner and to institutions who would like to analyze it. Privacy mechanisms protect the owner's data while releasing to analysts noisy versions of aggregate query results. But such strict protections of individual's data have not yet found wide use in practice. Instead, Internet companies, for example, commonly provide free services in return for valuable sensitive information from users, which they exploit and sometimes sell to third parties.    In this poster we propose a theoretical framework for assigning prices to both accurate and noisy query answers, as a function of their accuracy. Our framework adopts and extends key principles from both differential privacy and query pricing in data markets. We identify essential properties of the price function and study the structure of price functions meeting these criteria. We further distinguish between NP-complete and PTIME cases for computing a conforming price function with accurate query answers and characterize valid solutions for noisy price functions."	"Chao Li, Umass Amherst; Daniel Li, University of Washington; Gerome Miklau, University of Massachusetts Amherst; Dan Suciu, University of Washington"
Robust Distributed Stream Processing	"Distributed stream processing systems must function efficiently for data streams that fluctuate in their arrival rates and data distributions. Yet repeated and prohibitively expensive load re-allocation across machines may make these systems ineffective, potentially resulting in data loss or even system failure. To overcome this problem, we instead propose a load distribution (RLD) strategy that is robust to data fluctuations. RLD provides $\epsilon$-optimal query performance under load fluctuations without suffering from the performance penalty caused by load migration. RLD is based on three key strategies. First, we model robust distributed stream processing as a parametric query optimization problem. The notions of robust logical and robust physical plans then are overlays of this parameter space. Second, our Early-terminated Robust Partitioning ({\it ERP}) finds a set of robust logical plans, covering the parameter space, while minimizing the number of prohibitively expensive optimizer calls with a {\it probabilistic bound} on the space coverage. Third, our {\it OptPrune} algorithm maps the space-covering logical solution to a single robust physical plan tolerant to deviations in data statistics that maximizes the parameter space coverage at runtime. Our experimental study using stock market and sensor networks streams demonstrates that our RLD methodology consistently outperforms state-of-the-art solutions in terms of efficiency and effectiveness in highly fluctuating data stream environments."	"Chuan Lei, WPI; Elke Rundensteiner, WPI"
Semantic Indexing	"Current search engines use the inverted index to encode natural language. Unfortunately, the inverted index fails to capture the semantics of the language and thus it can only go as far as giving relevant hits. This project introduces semantic indexing scheme which can encode natural language without losing the semantic relationships."	"Anant Bhardwaj, MIT; David Karger, MIT; Samuel Madden, MIT"
StatusQuo: Making Familiar Abstractions Perform Using Program Analysis	"Modern web applications rely on databases for persistent storage, but the strong separation between the application and the database layer makes it difficult to satisfy end-to-end goals such as performance, reliability, and security. In this poster, we describe StatusQuo, a system that aims to address this problem by using program analysis and program synthesis enables the seamless movement of functionality between the database and application layer. It does so by taking functionality that was originally written as imperative code and translating it into relational queries that execute in the database. In addition, it makes runtime decisions about the optimal placement of computation to reduce data movement between the database and application server. In this poster, we describe promising empirical results from the two key technologies that make up StatusQuo and highlight some open research problems in order to achieve the end-to-end vision."	"Alvin  Cheung, MIT CSAIL; Owen Arden, Cornell University; Samuel Madden, MIT; Armando Solar-Lezama, MIT CSAIL; Andrew Myers, Cornell University"
The Future of Medical Data	"Abstract: Medicine is already becoming more dependent on the medical data contained within the medical record systems mandated by the Recovery Act. Doctors and medical practitioners have begun to shift into a data-based decision-making paradigm. The next great leap that data-driven medicine will take will involve assimilation of proteomic- and genomic-level databases into the medical record system itself. This will enable three key functions: the foundation of data mining/predictive modeling, better patient care via greater depth of knowledge, and the ability to tailor gene-based or protein-specific treatments to the patient. All of this requires massive database and data-driven enterprises: it can be argued that medicine is becoming a true data-focused field."	"Thomas Murakami-Brundage, University of Pennsylvania; Vince Frangiosa, University of Pennsylvania; Brian Wells, University of Pennsylvania"
The Penn Data Store and Medical Data Integration	"Abstract: As a premier research institution, the University of Pennsylvania harbors numerous databases: these run the gamut from clinical, research, and financial, to genomic and neurological. Integrating all these disparate data sources has become a massive endeavor at the University's Medical System. In order to accommodate the organization's vast needs for data, and to assist accomplishing the objective of become a data-driven enterprise, Penn built the Penn Data Store clinical data warehouse, one of 12 CCHIT-certified Data Warehouses within the USA designated for clinical research use. This warehouse is an ongoing project: it currently incorporates inpatient and outpatient data from eleven distinctly different medical record systems, and is constantly in the process of assimilating even more content from Penn's medical databases in order to directly contribute to medical research."	"Thomas Murakami-Brundage, University of Pennsylvania; Vince Frangiosa, University of Pennsylvania; Brian Wells, University of Pennsylvania; Brent Moore, University of Pennsylvania"
The Yin and Yang of Processing Data Warehousing Queries on GPU Devices	"Database community has made significant research efforts to  optimize query processing on GPUs in the past few years.  However,  we can hardly find that GPUs have been truly adopted in major data warehousing production systems.  To understand main reasons behind this fact,  we have conducted a comprehensive study to evaluate the performance of processing complex data warehousing queries  by varying query characteristics, software optimization techniques, and GPU hardware configurations.  Our study focuses on  the two fundamental components of query execution on GPUs,  PCIe data transfer and kernel execution,  aiming at gaining deep insights on  how they are affected by various factors from software to hardware.  Furthermore,  we also propose an analytical model to understand and predict the variations of the two factors.  Based on our study, we present our performance insights and prediction  for warehousing query execution on GPUs.  "	"Yuan Yuan, The Ohio State Univerisy; Rubao Li, The Ohio State University; Xiaodong Zhang, "
Using Counting Index Structure to Optimize Ranking-Related Queries	"Finding the rank of an entry in a sorted list of data is a common operation in a lot of applications. Take a leaderboard within gaming application as an example: it is common to have a scoreboard that keeps track of users' scores. Questions like, "what are the top-n users and their corresponding scores?", or "what is the rank of a certain user?" are often asked. While the first question is easy to answer with a simple SQL query with an ORDER BY and a LIMIT, the second one can be hard to answer efficiently.    By storing an extra counter for each tree node of the index indicating how many tree nodes are its children in total, we can compute its rank with O(logN) time complexity instead of O(N). For multi-node transactions on a distributed database, the global rank can still be got by summing up the number of tree nodes whose values are larger than its own. Space utilization can be tuned with the size of counter, which should not be a problem.     With this counting index feature, the Ranking-Related Queries are more than 1K time faster than original queries on a five million rows table."	"Xin Jia, Brown University; Ning Shi, VoltDB"
Workflow-aware Stream Processing with HIT	"Modern digital devices generate huge volumes of event data enabling real-time reactive applications ranging from supply chain tracking to emergency management. These lower-level events must be aggregated and composed into higher-level semantic activities to enable stream reasoning, process monitoring, dynamic process management and optimization at the right level of abstraction. To allow for compact but expressive stream processing queries, the workflow-determined and query-specific parts of the application semantics are separate dimensions in our approach. The workflow is captured by the Hierarchical Instantiating Timed automaton (HIT). The queries are formulated in the context of states of the automaton. In contrast to existing hierarchical timed workflow models, HIT allows for (1) access to data carried by events and event occurrence time intervals, (2) short but expressive queries, and (3) modeling an arbitrary number of hierarchical concurrent processes. HIT combines a compact human-comprehensible representation with strong formal foundations and expressiveness as required for realization of complex real-world scenarios. Exploiting the parallelizability of HIT specifications, our HIT prototype is now being developed as a scalable multi-threaded runtime execution infrastructure. Due to these features, the potential of HIT is promising.  "	"Olga Poppe, Worcester Polytechnic Inst."
